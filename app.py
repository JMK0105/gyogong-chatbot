import streamlit as st
import pandas as pd
import gspread
import json
from datetime import datetime
from google.oauth2 import service_account
from googleapiclient.discovery import build
import openai
import difflib
from fpdf import FPDF
import time
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt



# âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ êµìœ¡ê³µí•™ ê¸°ë°˜ì˜ í˜‘ë ¥í•™ìŠµì„ ì§€ì›í•˜ëŠ” ì§€ëŠ¥í˜• í”¼ë“œë°± ì±—ë´‡ì…ë‹ˆë‹¤.
ì´ íŒ€ì€ ì¤‘ë“± êµì‚¬ ëŒ€ìƒ ì›ê²© ì§ë¬´ì—°ìˆ˜ ì½˜í…ì¸ ì¸ ã€Œì—ë“€í…Œí¬ í™œìš© PBL ìˆ˜ì—… ì‹¤ì²œë²•ã€ì„ ì„¤ê³„í•˜ê³  ìˆìœ¼ë©°,
í•™ìƒë“¤ì€ ì‹¤ì œ êµìœ¡ í˜„ì¥ì—ì„œ ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì—… ì‚¬ë¡€ê°€ í¬í•¨ëœ ê°•ì˜ ì½˜í…ì¸ ë¥¼ ê°œë°œí•´ì•¼ í•©ë‹ˆë‹¤.

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ í‰ê°€ ê¸°ì¤€ì— ê¸°ë°˜í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤. íšŒì˜ ë‚´ìš©ì„ ë¶„ì„í•  ë•Œ ì´ ê¸°ì¤€ê³¼ ë¶€í•©í•˜ëŠ”ì§€ íŒë‹¨í•˜ê³ , ê·¸ì— ë”°ë¥¸ ë¶„ì„ê³¼ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”:

[ê³¼ì œ í‰ê°€ ê¸°ì¤€]
1. ê³¼ì • ì í•©ì„±: ì½˜í…ì¸  íë¦„ê³¼ ì»¨ì…‰ì´ â€˜ì¤‘ë“±êµì‚¬ ëŒ€ìƒ PBL ìˆ˜ì—… ì‹¤ì²œ ì—°ìˆ˜â€™ì— ì ì ˆí•œê°€?
2. ì‚¬ë¡€ì˜ êµ¬ì²´ì„±ê³¼ ì •í™•ì„±: ì œì‹œëœ ìˆ˜ì—… ì‚¬ë¡€ê°€ êµê³¼ ë° í•™ìŠµì ë§¥ë½ì— ë§ê³ , êµìœ¡ì  íƒ€ë‹¹ì„±ì„ ê°–ì¶”ì—ˆëŠ”ê°€?
3. ì ìš© ê°€ëŠ¥ì„±: êµì‚¬ê°€ ì‹¤ì œ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì„±ê³¼ í˜„ì‹¤ì„±ì„ ê°–ì¶”ì—ˆëŠ”ê°€?
4. ì½˜í…ì¸  êµ¬ì¡° ë° íë¦„: ê°•ì˜ì˜ ì „ì²´ êµ¬ì„±ê³¼ íë¦„ì´ ë…¼ë¦¬ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ê°€?
5. ë‚´ìš© ì „ë‹¬ë ¥: êµì‚¬ê°€ ë“£ê³  ì‰½ê²Œ ì´í•´í•˜ê³  ë”°ë¼í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì—ˆëŠ”ê°€?

ë‹¤ìŒ 7ê°€ì§€ ì˜ì—­ì— ë”°ë¼ íšŒì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì„¸ìš”. ê° í•­ëª© ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì†Œì œëª©ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ êµ¬ë¶„í•˜ì„¸ìš”.
í”¼ë“œë°±ì„ ì œê³µí•  ë•ŒëŠ” 'íŒ€ì›ë³„ ê¸°ì—¬ë„', 'ì˜í•œ ì ', 'ê°œì„ í•  ì ', 'ë‹¤ìŒ íšŒì˜ ì œì•ˆ'ì„ ìš”ì•½ í¬ì¸íŠ¸ë¡œ ì œê³µí•˜ì„¸ìš”.
ê° í•­ëª©ì€ 1~2ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ë˜, êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”.

7ê°€ì§€ ë¶„ì„ ì˜ì—­:
1. ì—­í•  ì •ë¦¬: ëˆ„ê°€ ì–´ë–¤ ì—­í• ì„ ë§¡ì•˜ëŠ”ì§€, ì°¸ì—¬ ê· í˜• ì—¬ë¶€, íŒ€ì›Œí¬ë¥¼ íŒë‹¨í•˜ì„¸ìš”.
2. ìê¸°ì¡°ì ˆ: ëª©í‘œ ì„¤ì •, ê³„íš ìˆ˜ë¦½, ì¼ì • ì¡°ìœ¨ ë“±ì˜ ìê¸°ì¡°ì ˆ ì „ëµì´ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
3. ë©”íƒ€ì¸ì§€: í˜„ì¬ í”„ë¡œì íŠ¸ ë‹¨ê³„ì— ëŒ€í•œ ì¸ì‹ ì—¬ë¶€ì™€ ì´ì— ë”°ë¥¸ ì „ëµì  ì‚¬ê³  ë˜ëŠ” í•„ìš” ì œì•ˆì´ ìˆì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
4. ì •ì„œì  í”¼ë“œë°±: íšŒì˜ ë¶„ìœ„ê¸°, ê¸ì •ì  ìƒí˜¸ì‘ìš© ì—¬ë¶€, ê²©ë ¤ ë°œì–¸ ë“±ì„ ë¶„ì„í•˜ê³ , ê°„ë‹¨í•œ ì‘ì› ë©”ì‹œì§€ë¥¼ ì œì‹œí•˜ì„¸ìš”.
5. ê°œì„  ì œì•ˆ: í˜„ì¬ ë…¼ì˜ë‚˜ ì‘ì—…ì˜ ë¶€ì¡±í•œ ì ì„ 1~2ê°€ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.
6. ì§„í–‰ ìš”ì•½: ì´ì „ íšŒì˜ ëŒ€ë¹„ ì§„ì „ ì‚¬í•­ì„ ìš”ì•½í•˜ì„¸ìš”.
7. ë‹¤ìŒ íšŒì˜ ì œì•ˆ: ë‹¤ìŒ íšŒì˜ì—ì„œ ë‹¤ë¤„ì•¼ í•  í•µì‹¬ ëª©í‘œë‚˜ ë…¼ì˜ ì£¼ì œë¥¼ ì œì‹œí•˜ì„¸ìš”.

[ì‘ë‹µ í˜•ì‹ ì˜ˆì‹œ]
ì—­í•  ì •ë¦¬:
...
ìê¸°ì¡°ì ˆ:
...
ë©”íƒ€ì¸ì§€:
...
ì •ì„œì  í”¼ë“œë°±:
...
ê°œì„  ì œì•ˆ:
...
ì§„í–‰ ìš”ì•½:
...
ë‹¤ìŒ íšŒì˜ ì œì•ˆ:
...
"""

st.set_page_config(page_title="êµê³µì´", layout="centered")
st.title("ğŸ¤– êµê³µì´ ì±—ë´‡")

team_codes = {
    "íŒ€test": "2025", "AíŒ€": "2026", "BíŒ€": "2024", "CíŒ€": "2023", "DíŒ€": "2022",
    "EíŒ€": "2021", "FíŒ€": "2020"
}

folder_ids = {
    "íŒ€test": "1-9vL1B5O2LoS1uyBzPK3Y6kIfOSKG-Fo",
    "AíŒ€": "1xdm-vXZ-bjch2bQWgHZ_GuQ8VjguCCaD",
    "BíŒ€": "1BFqy-38ZOFEvxvqPBwRo5-SOaVSoK-oL",
    "CíŒ€": "1Ey9nh0vICcDOtQrIQg0XbLEehqNIShYb",
    "DíŒ€": "1kAb13Qipe-0xw2o6WbLXLi2xrcqjuxoc",
    "EíŒ€": "1dkSXOSTMDewbt0oGj-FZvWHPCFpTe8vK",
    "FíŒ€": "17C8Yfjvr8d3kR1XLJtjfcx80xBjaON1p"
}

for key in ["authenticated", "team_name", "meeting_text", "result_text", "selected_file"]:
    if key not in st.session_state:
        st.session_state[key] = "" if key != "authenticated" else False

# âœ… íšŒì˜ ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
def load_team_history(creds, team_name):
    sh = gspread.authorize(creds).open_by_key("1LNKXL83dNvsHDOHEkw7avxKRsYWCiIIIYKUPiF1PZGY")
    worksheet = sh.sheet1
    data = worksheet.get_all_records()
    df = pd.DataFrame(data)
    df.columns = [str(col).strip() for col in df.columns]
    if 'ì‹œê°„' not in df.columns:
        return pd.DataFrame()
    df['ì‹œê°„'] = pd.to_datetime(df['ì‹œê°„'], errors='coerce')
    return df[df['íŒ€ëª…'] == team_name].sort_values(by='ì‹œê°„')

# âœ… ë¶„ì„ ê²°ê³¼ íŒŒì‹± í•¨ìˆ˜
def extract_structured_feedback(text):
    keys = ["ì—­í•  ì •ë¦¬", "ìê¸°ì¡°ì ˆ", "ë©”íƒ€ì¸ì§€", "ì •ì„œì  í”¼ë“œë°±", "ê°œì„  ì œì•ˆ", "ì§„í–‰ ìš”ì•½", "ë‹¤ìŒ íšŒì˜ ì œì•ˆ"]
    result = {k: "" for k in keys}
    for k in keys:
        if k in text:
            try:
                after = text.split(k)[1]
                for other in keys:
                    if other != k and other in after:
                        after = after.split(other)[0]
                result[k] = after.strip()
            except:
                result[k] = ""
    return result

# âœ… ì‹œíŠ¸ ì €ì¥
def save_to_sheet(gc, team_name, title, parsed, full_text=""):
    try:
        worksheet = gc.open_by_key("1LNKXL83dNvsHDOHEkw7avxKRsYWCiIIIYKUPiF1PZGY").sheet1
        worksheet.append_row([
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            team_name,
            title,
            parsed.get("ì—­í•  ì •ë¦¬", ""),
            parsed.get("ìê¸°ì¡°ì ˆ", ""),
            parsed.get("ë©”íƒ€ì¸ì§€", ""),
            parsed.get("ì •ì„œì  í”¼ë“œë°±", ""),
            parsed.get("ê°œì„  ì œì•ˆ", ""),
            parsed.get("ì§„í–‰ ìš”ì•½", ""),
            parsed.get("ë‹¤ìŒ íšŒì˜ ì œì•ˆ", ""),
            full_text  # âœ… ì „ì²´ íšŒì˜ë¡ ì¶”ê°€
            ])
        return True
    except Exception as e:
        st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# âœ… íšŒì˜ ìš”ì•½ ìš”ì  ì¶œë ¥
def display_summary_feedback(parsed):
    st.subheader("ğŸ“‹ íšŒì˜ë¡ í”¼ë“œë°±")
    st.markdown("### ğŸ€ íŒ€ì›Œí¬")
    st.markdown(parsed.get("ì—­í•  ì •ë¦¬", "").strip())

    st.markdown("### ğŸ‘ ì˜í•œ ì ")
    st.markdown(parsed.get("ìê¸°ì¡°ì ˆ", "").strip())
    st.markdown(parsed.get("ì •ì„œì  í”¼ë“œë°±", "").strip())

    st.markdown("### âš ï¸ ê°œì„ í•  ì ")
    st.markdown(parsed.get("ê°œì„  ì œì•ˆ", "").strip())
    st.markdown(parsed.get("ë©”íƒ€ì¸ì§€", "").strip())
    st.markdown(parsed.get("ì§„í–‰ ìš”ì•½", "").strip())

    st.markdown("### âœ¨ ë‹¤ìŒ íšŒì˜ ì œì•ˆ")
    st.markdown(parsed.get("ë‹¤ìŒ íšŒì˜ ì œì•ˆ", "").strip())

def add_dashboard(df):
    import altair as alt
    from gensim import corpora
    from gensim.models.ldamodel import LdaModel


    # âœ… íšŒì˜ë¡ ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    df = df.drop_duplicates(subset="íšŒì˜ë¡ ì œëª©", keep="last").reset_index(drop=True)

    st.header("ğŸ“Š íŒ€ íšŒì˜ ëŒ€ì‹œë³´ë“œ")

    def clean_korean_text(text):
        import re
        text = re.sub(r"[^ê°€-í£\s]", "", text)
        words = text.split()
        stopwords = set([
            "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "ë•Œë¬¸ì—", "ë“±",
            "ìœ„í•œ", "í•˜ëŠ”", "ìˆë‹¤", "ìˆìŠµë‹ˆë‹¤", "ì´ë‹¤", "ëœë‹¤", "ê°™ë‹¤",
            "ê²½ìš°", "ì •ë„", "ë¶€ë¶„", "ë‚´ìš©", "ë°©ë²•", "í™œë™", "ê²°ê³¼", "ì œì‹œ",
            "ëŒ€í•œ", "ëŒ€í•´", "ì´ì—", "ë¡œì„œ",
            "ìœ¼ë¡œ", "ê²ƒì´", "ë¡œë¶€í„°", "ì—ê²Œ", "ëœë‹¤ë©´", "í•©ë‹ˆë‹¤", "ìˆì–´ìš”"
        ])
        return [w for w in words if len(w) > 1 and w not in stopwords and len(w) <= 6]

    df["ë¶„ì„í…ìŠ¤íŠ¸"] = df["ì „ì²´ íšŒì˜ë¡"].fillna("")

    # 1ï¸âƒ£ ì›Œë“œí´ë¼ìš°ë“œ & í‚¤ì›Œë“œ ë³€í™” ì¶”ì´
    with st.expander("ğŸ” íšŒì°¨ë³„ í•µì‹¬ í‚¤ì›Œë“œ", expanded=False):
        col1, col2 = st.columns([1, 1.5])

        with col1:
                if len(df) == 1:
                    selected_idx = 1
                else:
                    selected_idx = st.slider("WordCloud íšŒì°¨ ì„ íƒ", 1, len(df), 1, key="wordcloud_slider")
                text = " ".join(clean_korean_text(df.iloc[selected_idx - 1]["ë¶„ì„í…ìŠ¤íŠ¸"]))
                if not text.strip():
                    st.info("âš ï¸ í•´ë‹¹ íšŒì°¨ì—ëŠ” í‘œì‹œí•  í‚¤ì›Œë“œê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                else:
                    wordcloud = WordCloud(
                        font_path="fonts/malgun.ttf",
                        background_color='white',
                        width=600,
                        height=400,
                        max_words=50,
                        max_font_size=90,
                        prefer_horizontal=0.9,
                        colormap='Dark2'
                    ).generate(text)
                    fig1, ax1 = plt.subplots(figsize=(6, 4))
                    ax1.imshow(wordcloud, interpolation='bilinear')
                    ax1.axis("off")
                    st.pyplot(fig1)

        with col2:
            tokenized = df["ë¶„ì„í…ìŠ¤íŠ¸"].apply(clean_korean_text)
            all_words = [word for row in tokenized for word in row if len(word) <= 6]
            top_keywords = [kw for kw, _ in Counter(all_words).most_common(4)]
            trend_data = [[row.count(kw) for kw in top_keywords] for row in tokenized]
            trend_df = pd.DataFrame(trend_data, columns=top_keywords)
            trend_df["íšŒì°¨"] = [f"{i+1}íšŒì°¨" for i in range(len(df))]
            trend_df_melted = trend_df.melt(id_vars="íšŒì°¨", var_name="í‚¤ì›Œë“œ", value_name="ë¹ˆë„")

            chart = alt.Chart(trend_df_melted).mark_line(point=True).encode(
                x=alt.X("íšŒì°¨:N", title="íšŒì˜ íšŒì°¨", axis=alt.Axis(labelAngle=0)),
                y=alt.Y("ë¹ˆë„:Q", title="ë“±ì¥ ë¹ˆë„ìˆ˜", scale=alt.Scale(domain=[0, trend_df_melted["ë¹ˆë„"].max() + 1])),
                color=alt.Color("í‚¤ì›Œë“œ:N", title="ì£¼ìš” í‚¤ì›Œë“œ"),
                tooltip=["íšŒì°¨", "í‚¤ì›Œë“œ", "ë¹ˆë„"]
            ).properties(
                title="íšŒì°¨ë³„ ì£¼ìš” í‚¤ì›Œë“œ ë“±ì¥ ë¹ˆë„ ë³€í™”",
                width=500, height=300
            )
            st.altair_chart(chart, use_container_width=True)

    # 2ï¸âƒ£ LDA ë¶„ì„ & ìš”ì•½
    with st.expander("ğŸ§  íšŒì˜ë¡ í…ìŠ¤íŠ¸ LDA ë¶„ì„", expanded=False):
        selected_indexes = st.multiselect("ë¶„ì„í•  íšŒì°¨ ì„ íƒ", df.index, format_func=lambda i: df.loc[i, "íšŒì˜ë¡ ì œëª©"] or f"{i+1}íšŒì°¨")

        if selected_indexes:
            selected_texts = df.loc[selected_indexes, "ë¶„ì„í…ìŠ¤íŠ¸"].apply(clean_korean_text).tolist()
            dictionary = corpora.Dictionary(selected_texts)
            corpus = [dictionary.doc2bow(text) for text in selected_texts]

            if len(dictionary) > 0 and len(corpus) > 0:
                lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=42)

                topic_keywords = []
                for i in range(3):
                    for word, prob in lda_model.show_topic(i, topn=5):
                        topic_keywords.append({"í† í”½": f"í† í”½ {i+1}", "í‚¤ì›Œë“œ": word, "í™•ë¥ ": prob})

                topic_df = pd.DataFrame(topic_keywords)
                chart = alt.Chart(topic_df).mark_bar().encode(
                    x=alt.X("í† í”½:N", title="í† í”½"),
                    y=alt.Y("í™•ë¥ :Q", title="ë¹„ì¤‘"),
                    color=alt.Color("í‚¤ì›Œë“œ:N"),
                    tooltip=["í† í”½", "í‚¤ì›Œë“œ", "í™•ë¥ "]
                ).properties(width=700, height=400)
                st.altair_chart(chart, use_container_width=True)

                # 3ï¸âƒ£ GPT ìš”ì•½
                try:
                    topic_summaries = []
                    for i in range(3):
                        keywords = ", ".join([word for word, _ in lda_model.show_topic(i, topn=5)])
                        topic_summaries.append(f"í† í”½ {i+1}: {keywords}")

                    summary_prompt = f"""
ë‹¤ìŒì€ ì‹¤ì œ ìˆ˜ì—… ì‚¬ë¡€ê°€ í¬í•¨ëœã€Œì—ë“€í…Œí¬ í™œìš© PBL ìˆ˜ì—… ì‹¤ì²œë²•ã€ì—°ìˆ˜ì„ ì„¤ê³„í•˜ëŠ” íšŒì˜ ë‚´ìš©ì—ì„œ LDAë¶„ì„ì„ í†µí•´ ì¶”ì¶œëœ ì£¼ìš” í† í”½ì…ë‹ˆë‹¤.
ê° í† í”½ì€ íšŒì˜ë¡ì—ì„œ ì˜ë¯¸ìˆê²Œ ë“±ì¥í•˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

{chr(10).join(topic_summaries)}

ì´ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ íšŒì˜ì—ì„œ ì–´ë–¤ ì£¼ì œê°€ ë…¼ì˜ë˜ì—ˆëŠ”ì§€ 3ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
í•­ëª©ë§ˆë‹¤ ì´ëª¨ì§€ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”.
"""

                    openai_client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

                    topic_response = openai_client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "ë‹¹ì‹ ì€ êµìœ¡ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ì¡°ë ¥ìì…ë‹ˆë‹¤."},
                            {"role": "user", "content": summary_prompt}
                        ]
                    )
                    summary_text = topic_response.choices[0].message.content
                    st.markdown("### ğŸ§  ì´ë²ˆ íšŒì˜ì—ì„œ ë…¼ì˜ëœ ì£¼ì œ ìš”ì•½")
                    st.info(summary_text)

                except Exception as e:
                    st.warning(f"í† í”½ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")

        else:
            st.info("âš ï¸ ì„ íƒëœ íšŒì°¨ì— ëŒ€í•œ LDA ëª¨ë¸ë§ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


            
# âœ… ì¸ì¦ ë° íšŒì˜ë¡ ì„ íƒ
code_input = st.text_input("âœ… íŒ€ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
if code_input:
    team_name = next((team for team, code in team_codes.items() if code_input == code), None)
    if team_name:
        st.session_state.authenticated = True
        st.session_state.team_name = team_name
        st.success(f"ğŸ‰ ì¸ì¦ ì™„ë£Œ: {team_name}")

# âœ… ë³¸ë¬¸ ì‹¤í–‰ ë¡œì§
if st.session_state.authenticated:
    team_name = st.session_state.team_name
    folder_id = folder_ids[team_name]

    creds_info = json.loads(st.secrets["google"]["GOOGLE_SERVICE_ACCOUNT"])
    scopes = [
        'https://www.googleapis.com/auth/spreadsheets',
        'https://www.googleapis.com/auth/drive.readonly',
        'https://www.googleapis.com/auth/documents.readonly'
    ]
    creds = service_account.Credentials.from_service_account_info(creds_info, scopes=scopes)
    gc = gspread.authorize(creds)
    drive_service = build('drive', 'v3', credentials=creds)
    docs_service = build('docs', 'v1', credentials=creds)
    openai_client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

    team_df = load_team_history(creds, team_name)
    if not team_df.empty:
        add_dashboard(team_df)

    results = drive_service.files().list(
        q=f"'{folder_id}' in parents and mimeType='application/vnd.google-apps.document'",
        pageSize=10,
        fields="files(id, name, createdTime)"
    ).execute()
    files = results.get('files', [])

    if files:
        file_dict = {f["name"]: f["id"] for f in sorted(files, key=lambda x: x['createdTime'])}
        selected_file = st.selectbox("ğŸ“ íšŒì˜ë¡ íšŒì°¨ ì„ íƒ", list(file_dict.keys()))
        st.session_state.selected_file = selected_file

        if st.button("ğŸ” íšŒì˜ë¡ ë¶„ì„ ì‹œì‘", disabled=st.session_state.get("button_disabled", False)):
            st.session_state["show_dashboard"] = False  # âœ… ëŒ€ì‹œë³´ë“œ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.button_disabled = True
            time.sleep(2)

            try:
                doc = docs_service.documents().get(documentId=file_dict[selected_file]).execute()
                elements = doc.get("body", {}).get("content", [])
                meeting_text = ''.join(
                    elem['textRun']['content']
                    for v in elements if 'paragraph' in v
                    for elem in v['paragraph'].get('elements', []) if 'textRun' in elem
                )
                st.session_state.meeting_text = meeting_text


                if "context_summary" not in st.session_state:
                    st.session_state.context_summary = "\n".join([
                        f"[{row['ì‹œê°„']}] {row.get('íšŒì˜ë¡ ì œëª©', '')}" for _, row in team_df.iterrows()
                    ])
                context_summary = st.session_state.context_summary
                
                if team_df.shape[0] > 0:
                    last_text = str(team_df.iloc[-1].get("ê°œì„  ì œì•ˆ", "")) + str(team_df.iloc[-1].get("ì§„í–‰ ìš”ì•½", ""))
                    similarity = difflib.SequenceMatcher(None, meeting_text.strip(), last_text.strip()).ratio()
                    if similarity >= 0.9:
                        st.info("âš ï¸ ì´ì „ íšŒì˜ì™€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤. ë™ì¼ íšŒì˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                with st.spinner("GPTê°€ íšŒì˜ë¡ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                    response = openai_client.chat.completions.create(
                        model="gpt-4-turbo",
                        messages=[
                            {"role": "system", "content": SYSTEM_PROMPT},
                            {"role": "user", "content": f"[ê³¼ê±° íšŒì˜ ìš”ì•½]\n{context_summary}\n\n[ì´ë²ˆ íšŒì˜ ë‚´ìš©]\n{meeting_text}"}
                        ]
                    )
                    result_text = response.choices[0].message.content
                    st.session_state.result_text = result_text
                    st.success("âœ… ë¶„ì„ ì™„ë£Œ!")

                    parsed = extract_structured_feedback(result_text)
                    if parsed:
                        already_saved = pd.DataFrame()
                        if not team_df.empty:
                        # âœ… ì´ë¯¸ ë™ì¼í•œ ì œëª©+ë³¸ë¬¸ì´ ì €ì¥ëœ ê²½ìš° ì €ì¥ ìƒëµ
                            already_saved = team_df[
                                (team_df["íšŒì˜ë¡ ì œëª©"] == selected_file) &
                                (team_df["ì „ì²´ íšŒì˜ë¡"] == meeting_text)
                            ]
                    
                        if not already_saved.empty:
                            st.info(f"âœ… ë™ì¼í•œ íšŒì˜ë¡ ë‚´ìš©ì„ ë¶„ì„í•œ ì´ë ¥ì´ ìˆìŠµë‹ˆë‹¤.")
                        else:
                            if save_to_sheet(gc, team_name, selected_file, parsed, meeting_text):
                                st.success("ğŸ“Œ íšŒì˜ë¡ ë‚´ìš©ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        display_summary_feedback(parsed)

                        # âœ… GPT ê¸°ë°˜ íŒ€ì›ë³„ ê¸°ì—¬ë„ ì¶”ì • ë° ì‹œê°í™”
                        st.subheader("ğŸ‘¥ GPT ê¸°ë°˜ íŒ€ì›ë³„ ê¸°ì—¬ë„")
                        with st.expander("ğŸ“ˆ íŒ€ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„"):
                            try:
                                contribution_prompt = f"""
                        ë‹¤ìŒì€ íšŒì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ íšŒì˜ì—ì„œ ë“±ì¥í•˜ëŠ” ì°¸ì—¬ì(ì´ë¦„)ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ, ê° ì¸ë¬¼ì´ íšŒì˜ì—ì„œ ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ë¥¼ 100% ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •í•˜ì—¬ 
                        JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë¨¼ì € ì¶œë ¥í•˜ê³ , ê·¸ ë‹¤ìŒ ê° ê¸°ì—¬ë„ì— ëŒ€í•œ ê°„ë‹¨í•œ í•´ì„ì„ 2ì¤„ ì´ë‚´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                        ì•„ë˜ ë‘ ê°€ì§€ í•­ëª©ì„ ìˆœì„œëŒ€ë¡œ ì œê³µí•˜ì„¸ìš”:
                        1. ê¸°ì—¬ë„ ë¹„ìœ¨ (JSON í˜•ì‹)
                        2. ê° íŒ€ì›ì´ ì–´ë–¤ ì—­í• ì„ í–ˆëŠ”ì§€, ì™œ í•´ë‹¹ ê¸°ì—¬ë„ë¡œ íŒë‹¨í–ˆëŠ”ì§€ ê°„ë‹¨íˆ í•´ì„
                        
                        [íšŒì˜ ë‚´ìš©]
                        {meeting_text}
                        """
                                contribution_response = openai_client.chat.completions.create(
                                    model="gpt-3.5-turbo",
                                    messages=[
                                        {"role": "system", "content": "ë‹¹ì‹ ì€ íŒ€ íšŒì˜ì—ì„œ íŒ€ì›ë³„ ê¸°ì—¬ë„ë¥¼ ë¶„ì„í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                                        {"role": "user", "content": contribution_prompt}
                                    ]
                                )

                                import re
                                raw_text = contribution_response.choices[0].message.content.strip()

                                # ğŸ¯ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì¤‘ê´„í˜¸ ë¸”ë¡ë§Œ)
                                json_str_match = re.search(r"\{.*\}", raw_text, re.DOTALL)
                                if not json_str_match:
                                    raise ValueError("JSON í˜•ì‹ì´ ì‘ë‹µì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                contribution_json = json.loads(json_str_match.group())

                                # í•´ì„ í…ìŠ¤íŠ¸ë§Œ ë”°ë¡œ ì¶”ì¶œ
                                explanation_match = re.split(r"\}\s*", raw_text, maxsplit=1)
                                explanation_text = explanation_match[1].strip() if len(explanation_match) > 1 else "í•´ì„ì´ ì—†ìŠµë‹ˆë‹¤."

                                # ğŸ¯ ì‹œê°í™”
                                from matplotlib import font_manager
                                # í•œê¸€ í°íŠ¸ ê²½ë¡œ ì§€ì • (ë¡œì»¬ì— ìˆì„ ê²½ìš° ê²½ë¡œ ìˆ˜ì • ê°€ëŠ¥)
                                font_path = "fonts/malgun.ttf"  # ë˜ëŠ” ì ˆëŒ€ ê²½ë¡œ
                                font_prop = font_manager.FontProperties(fname=font_path)

                                st.markdown("#### ğŸ” ì¶”ì •ëœ ê¸°ì—¬ë„ ë¶„í¬")
                                fig, ax = plt.subplots()
                                wedges, texts, autotexts = ax.pie(contribution_json.values(), 
                                                                  labels=contribution_json.keys(), autopct='%1.1f%%', startangle=90, textprops={'fontsize': 12})

                                # í°íŠ¸ ì„¤ì • ì ìš©
                                for t in texts + autotexts:
                                    t.set_fontproperties(font_prop)
                                    
                                ax.axis('equal')
                                st.pyplot(fig)
                                
                                # í•´ì„ ì¶œë ¥
                                st.markdown("#### ğŸ’¬ ê¸°ì—¬ë„ í•´ì„")
                                st.info(explanation_text)
                        
                            
                            except Exception as e:
                                st.warning(f"âš ï¸ ê¸°ì—¬ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            except openai.RateLimitError:
                st.warning("â±ï¸ ìš”ì²­ì´ ë„ˆë¬´ ë¹ ë¦…ë‹ˆë‹¤. 5ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            finally:
                st.session_state.button_disabled = False

        import re
        class UnicodePDF(FPDF):
            def __init__(self):
                super().__init__()
                self.add_page()
                self.add_font("malgun", "", "fonts/malgun.ttf", uni=True)  # âœ… í°íŠ¸ ê²½ë¡œëŠ” ì§ì ‘ ì¶”ê°€í•´ì•¼ í•¨
                self.set_font("malgun", size=12)

            def add_text(self, text):
                text = re.sub(r'[\U00010000-\U0010ffff]', '', text)  # ì´ëª¨ì§€ ì œê±°
                for line in text.split('\n'):
                    self.multi_cell(0, 10, line)
 
        if st.session_state.result_text:
           if st.button("ğŸ“„ ë¶„ì„ ê²°ê³¼ PDFë¡œ ì €ì¥"):
               filename = f"{selected_file}_ë¶„ì„ê²°ê³¼.pdf"
               pdf = UnicodePDF()
               pdf.add_text(st.session_state.result_text)
               pdf.output(filename)
               with open(filename, "rb") as f:
                   st.download_button("â¬‡ï¸ PDF ë‹¤ìš´ë¡œë“œ", f, file_name=filename) 
